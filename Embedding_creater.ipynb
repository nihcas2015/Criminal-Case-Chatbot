{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f702c79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 933 documents. JSON and FAISS index saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ======= CONFIG =======\n",
    "SUMMARY_FOLDER = \"summary\"\n",
    "JSON_OUTPUT = \"judgments.json\"\n",
    "FAISS_INDEX = \"judgments.index\"\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "# ======================\n",
    "\n",
    "# Load embedding model\n",
    "model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "\n",
    "# Regex patterns for flexible section detection\n",
    "section_patterns = {\n",
    "    \"case_facts\": re.compile(r\"(case\\s*facts)\", re.IGNORECASE),\n",
    "    \"issues\": re.compile(r\"(issues?)\", re.IGNORECASE),\n",
    "    \"arguments\": re.compile(r\"(arguments?|reasoning)\", re.IGNORECASE),\n",
    "    \"decision\": re.compile(r\"(decision|holding|judgment)\", re.IGNORECASE)\n",
    "}\n",
    "\n",
    "def extract_sections_fuzzy(text):\n",
    "    sections = {k: \"\" for k in section_patterns.keys()}\n",
    "    current_key = None\n",
    "\n",
    "    for line in text.splitlines():\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # Check if line matches any section heading\n",
    "        matched = False\n",
    "        for key, pattern in section_patterns.items():\n",
    "            if pattern.search(line):  # If heading found\n",
    "                current_key = key\n",
    "                matched = True\n",
    "                break\n",
    "        \n",
    "        # If line is not heading, add to current section\n",
    "        if current_key and not matched:\n",
    "            sections[current_key] += line + \" \"\n",
    "\n",
    "    return sections\n",
    "\n",
    "# Process all summaries in folder\n",
    "documents = []\n",
    "for file_name in os.listdir(SUMMARY_FOLDER):\n",
    "    if not file_name.endswith(\".txt\"):\n",
    "        continue\n",
    "\n",
    "    file_path = os.path.join(SUMMARY_FOLDER, file_name)\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    sections = extract_sections_fuzzy(text)\n",
    "    doc_id = os.path.splitext(file_name)[0]\n",
    "\n",
    "    doc = {\n",
    "        \"id\": doc_id,\n",
    "        \"case_facts\": sections[\"case_facts\"].strip(),\n",
    "        \"issues\": sections[\"issues\"].strip(),\n",
    "        \"arguments\": sections[\"arguments\"].strip(),\n",
    "        \"decision\": sections[\"decision\"].strip()\n",
    "    }\n",
    "    documents.append(doc)\n",
    "\n",
    "# Save as JSON\n",
    "with open(JSON_OUTPUT, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(documents, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Build FAISS index\n",
    "texts = [doc[\"case_facts\"] + \" \" + doc[\"issues\"] + \" \" + doc[\"arguments\"] + \" \" + doc[\"decision\"] for doc in documents]\n",
    "embeddings = model.encode(texts, convert_to_numpy=True)\n",
    "\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n",
    "\n",
    "faiss.write_index(index, FAISS_INDEX)\n",
    "\n",
    "print(f\"Processed {len(documents)} documents. JSON and FAISS index saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08e4cd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created and saved 511 IPC sections:\n",
      "Index: ipc.index\n",
      "Metadata: ipc.json\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# --- Configuration (Update as needed) ---\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "CSV_FILE = \"ipc_formatted_clean.csv\"\n",
    "INDEX_FILE = \"ipc.index\"\n",
    "METADATA_FILE = \"ipc.json\"\n",
    "\n",
    "# --- 1. Load and Prepare Data ---\n",
    "\n",
    "# Attempt to load the file using a permissive encoding to avoid Unicode errors\n",
    "ipc = pd.read_csv(CSV_FILE, encoding='cp1252')\n",
    "\n",
    "# Fix the 'AttributeError: 'float' object has no attribute 'strip'' by filling NaN values.\n",
    "# This is crucial for robust string processing.\n",
    "columns_to_clean = ['Section Code', 'Description', 'Punishment/Consequence']\n",
    "for col in columns_to_clean:\n",
    "    if col in ipc.columns:\n",
    "        ipc[col] = ipc[col].fillna('').astype(str)\n",
    "\n",
    "# --- 2. Format Data for Embeddings (The \"final_text\" creation logic) ---\n",
    "\n",
    "def format_section(row):\n",
    "    \"\"\"Formats a single row into a coherent document block for the Sentence Transformer.\"\"\"\n",
    "    section_code = row[\"Section Code\"].strip()\n",
    "    description = row[\"Description\"].strip()\n",
    "    punishment = row[\"Punishment/Consequence\"].strip()\n",
    "    \n",
    "    # Create the clean, dense text block for semantic search\n",
    "    return f\"\"\"\n",
    "SECTION: {section_code}\n",
    "DESCRIPTION: {description}\n",
    "PUNISHMENT: {punishment}\n",
    "\"\"\"\n",
    "\n",
    "# Apply the function to the DataFrame to get a list of formatted documents\n",
    "formatted_series = ipc.apply(format_section, axis=1)\n",
    "\n",
    "# Documents for Embedding (the core text) and Metadata (for lookup)\n",
    "documents = formatted_series.tolist()\n",
    "metadata = ipc.rename(columns={'Section Code': 'section_no', 'Punishment/Consequence': 'punishment_raw'}).to_dict('records')\n",
    "\n",
    "# --- 3. Create Embeddings and FAISS Index ---\n",
    "\n",
    "# Load embedding model\n",
    "embedder = SentenceTransformer(EMBEDDING_MODEL)\n",
    "\n",
    "# Generate embeddings for all documents\n",
    "embeddings = embedder.encode(documents)\n",
    "\n",
    "# Convert to a float32 NumPy array, as required by FAISS\n",
    "embeddings_np = np.array(embeddings).astype(\"float32\")\n",
    "dimension = embeddings_np.shape[1]\n",
    "\n",
    "# Create a FAISS Index (IndexFlatL2 uses Euclidean distance for search)\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings_np)\n",
    "\n",
    "# --- 4. Save Index and Metadata ---\n",
    "\n",
    "# Save the FAISS index\n",
    "faiss.write_index(index, INDEX_FILE)\n",
    "\n",
    "# Save the metadata (original data) mapped by its index position\n",
    "with open(METADATA_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"Successfully created and saved {len(documents)} IPC sections:\")\n",
    "print(f\"Index: {INDEX_FILE}\")\n",
    "print(f\"Metadata: {METADATA_FILE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
