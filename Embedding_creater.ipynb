{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f702c79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1: Reading and parsing judgment files\n",
      "============================================================\n",
      "Read: 30.txt\n",
      "  - Case facts: 615 chars\n",
      "  - Issues: 266 chars\n",
      "  - Arguments: 873 chars\n",
      "  - Decision: 192 chars\n",
      "Read: 38.txt\n",
      "  - Case facts: 497 chars\n",
      "  - Issues: 193 chars\n",
      "  - Arguments: 534 chars\n",
      "  - Decision: 126 chars\n",
      "Read: 7.txt\n",
      "  - Case facts: 501 chars\n",
      "  - Issues: 307 chars\n",
      "  - Arguments: 736 chars\n",
      "  - Decision: 237 chars\n",
      "\n",
      "Total files read: 3\n",
      "\n",
      "============================================================\n",
      "STEP 2: Creating section embeddings\n",
      "============================================================\n",
      "\n",
      "Loading model: sentence-transformers/all-MiniLM-L6-v2\n",
      "\n",
      "Generated 3 judgment vectors\n",
      "Each vector contains 4 section embeddings concatenated\n",
      "\n",
      "============================================================\n",
      "STEP 3: Creating FAISS index\n",
      "============================================================\n",
      "\n",
      "Creating FAISS index (dimension: 1536)...\n",
      "FAISS index created with 3 vectors\n",
      "Each vector represents 4 sections (dimension = 384.0 per section)\n",
      "\n",
      "============================================================\n",
      "STEP 4: Saving index and metadata\n",
      "============================================================\n",
      "\n",
      "FAISS index saved to: judgments.index\n",
      "Metadata saved to: metadata.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "\n",
    "def parse_judgment_sections(content):\n",
    "    \"\"\"\n",
    "    Parse judgment text into sections: Case facts, Issues, Arguments/Reasoning, Decision/Holding.\n",
    "    \n",
    "    Args:\n",
    "        content: Full text content of judgment\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with sections as keys\n",
    "    \"\"\"\n",
    "    sections = {\n",
    "        'case_facts': '',\n",
    "        'issues': '',\n",
    "        'arguments': '',\n",
    "        'decision': ''\n",
    "    }\n",
    "    \n",
    "    # Define section patterns (case-insensitive)\n",
    "    patterns = {\n",
    "        'case_facts': r'Case facts?:(.+?)(?=Issues?:|Arguments?|Decision|Holding|$)',\n",
    "        'issues': r'Issues?:(.+?)(?=Arguments?|Reasoning|Decision|Holding|$)',\n",
    "        'arguments': r'(?:Arguments?|Reasoning)[:/](.+?)(?=Decision|Holding|$)',\n",
    "        'decision': r'(?:Decision|Holding)[:/](.+?)$'\n",
    "    }\n",
    "    \n",
    "    for section_name, pattern in patterns.items():\n",
    "        match = re.search(pattern, content, re.IGNORECASE | re.DOTALL)\n",
    "        if match:\n",
    "            sections[section_name] = match.group(1).strip()\n",
    "    \n",
    "    return sections\n",
    "\n",
    "def read_judgment_files(folder_path):\n",
    "    \"\"\"\n",
    "    Read all text files from a folder and parse them into sections.\n",
    "    \n",
    "    Args:\n",
    "        folder_path: Path to folder containing text files\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries with filename and sections\n",
    "    \"\"\"\n",
    "    judgments = []\n",
    "    folder = Path(folder_path)\n",
    "    \n",
    "    if not folder.exists():\n",
    "        raise FileNotFoundError(f\"Folder not found: {folder_path}\")\n",
    "    \n",
    "    # Read all .txt files\n",
    "    for file_path in folder.glob(\"*.txt\"):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read().strip()\n",
    "                if content:\n",
    "                    sections = parse_judgment_sections(content)\n",
    "                    judgments.append({\n",
    "                        'filename': file_path.name,\n",
    "                        'sections': sections\n",
    "                    })\n",
    "                    print(f\"Read: {file_path.name}\")\n",
    "                    print(f\"  - Case facts: {len(sections['case_facts'])} chars\")\n",
    "                    print(f\"  - Issues: {len(sections['issues'])} chars\")\n",
    "                    print(f\"  - Arguments: {len(sections['arguments'])} chars\")\n",
    "                    print(f\"  - Decision: {len(sections['decision'])} chars\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path.name}: {e}\")\n",
    "    \n",
    "    return judgments\n",
    "\n",
    "def create_section_embeddings(judgments, model_name='sentence-transformers/all-MiniLM-L6-v2'):\n",
    "    \"\"\"\n",
    "    Create embeddings for each section, but maintain as single block per judgment.\n",
    "    \n",
    "    Args:\n",
    "        judgments: List of judgment dictionaries\n",
    "        model_name: Name of the sentence transformer model\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (embeddings_array, metadata_list)\n",
    "    \"\"\"\n",
    "    print(f\"\\nLoading model: {model_name}\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    all_embeddings = []\n",
    "    metadata_list = []\n",
    "    \n",
    "    section_names = ['case_facts', 'issues', 'arguments', 'decision']\n",
    "    \n",
    "    for judgment in judgments:\n",
    "        # Create embeddings for each section\n",
    "        section_embeddings = []\n",
    "        \n",
    "        for section_name in section_names:\n",
    "            section_text = judgment['sections'][section_name]\n",
    "            if section_text:\n",
    "                embedding = model.encode(section_text, convert_to_numpy=True)\n",
    "                section_embeddings.append(embedding)\n",
    "            else:\n",
    "                # If section is empty, use zero vector\n",
    "                dimension = model.get_sentence_embedding_dimension()\n",
    "                section_embeddings.append(np.zeros(dimension))\n",
    "        \n",
    "        # Stack section embeddings horizontally (concatenate)\n",
    "        # This creates a single vector representing all 4 sections\n",
    "        combined_embedding = np.concatenate(section_embeddings)\n",
    "        \n",
    "        all_embeddings.append(combined_embedding)\n",
    "        \n",
    "        # Store metadata - only sections, no duplication\n",
    "        metadata_list.append({\n",
    "            'filename': judgment['filename'],\n",
    "            'sections': judgment['sections']\n",
    "        })\n",
    "    \n",
    "    embeddings_array = np.array(all_embeddings)\n",
    "    \n",
    "    # Normalize embeddings for cosine similarity\n",
    "    embeddings_array = embeddings_array / np.linalg.norm(embeddings_array, axis=1, keepdims=True)\n",
    "    \n",
    "    return embeddings_array, metadata_list, model\n",
    "\n",
    "def create_faiss_index(embeddings):\n",
    "    \"\"\"\n",
    "    Create FAISS index from embeddings.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: numpy array of embeddings\n",
    "        \n",
    "    Returns:\n",
    "        FAISS index\n",
    "    \"\"\"\n",
    "    dimension = embeddings.shape[1]\n",
    "    print(f\"\\nCreating FAISS index (dimension: {dimension})...\")\n",
    "    \n",
    "    # Using IndexFlatIP for cosine similarity (after normalization)\n",
    "    index = faiss.IndexFlatIP(dimension)\n",
    "    index.add(embeddings.astype('float32'))\n",
    "    \n",
    "    print(f\"FAISS index created with {index.ntotal} vectors\")\n",
    "    print(f\"Each vector represents 4 sections (dimension = {dimension/4} per section)\")\n",
    "    \n",
    "    return index\n",
    "\n",
    "def save_index_and_metadata(index, metadata, output_dir='faiss_index'):\n",
    "    \"\"\"\n",
    "    Save FAISS index and metadata to disk (JSON format for metadata).\n",
    "    \n",
    "    Args:\n",
    "        index: FAISS index\n",
    "        metadata: List of metadata dictionaries\n",
    "        output_dir: Directory to save files\n",
    "    \"\"\"\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Save FAISS index\n",
    "    index_file = output_path / 'judgments.index'\n",
    "    faiss.write_index(index, str(index_file))\n",
    "    print(f\"\\nFAISS index saved to: {index_file}\")\n",
    "    \n",
    "    # Save metadata as JSON\n",
    "    metadata_file = output_path / 'metadata.json'\n",
    "    with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Metadata saved to: {metadata_file}\")\n",
    "\n",
    "def load_index_and_metadata(input_dir='faiss_index'):\n",
    "    \"\"\"\n",
    "    Load FAISS index and metadata from disk.\n",
    "    \n",
    "    Args:\n",
    "        input_dir: Directory containing saved files\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (index, metadata)\n",
    "    \"\"\"\n",
    "    input_path = Path(input_dir)\n",
    "    \n",
    "    # Load FAISS index\n",
    "    index_file = input_path / 'judgments.index'\n",
    "    index = faiss.read_index(str(index_file))\n",
    "    print(f\"FAISS index loaded from: {index_file}\")\n",
    "    \n",
    "    # Load metadata from JSON\n",
    "    metadata_file = input_path / 'metadata.json'\n",
    "    with open(metadata_file, 'r', encoding='utf-8') as f:\n",
    "        metadata = json.load(f)\n",
    "    print(f\"Metadata loaded from: {metadata_file}\")\n",
    "    \n",
    "    return index, metadata\n",
    "\n",
    "def search_by_section(query, section_type, index, metadata, model, top_k=5):\n",
    "    \"\"\"\n",
    "    Search for similar judgments by focusing on a specific section.\n",
    "    \n",
    "    Args:\n",
    "        query: Query text\n",
    "        section_type: 'case_facts', 'issues', 'arguments', or 'decision'\n",
    "        index: FAISS index\n",
    "        metadata: List of metadata dictionaries\n",
    "        model: SentenceTransformer model\n",
    "        top_k: Number of top results to return\n",
    "        \n",
    "    Returns:\n",
    "        List of tuples: (filename, similarity_score, relevant_section_text)\n",
    "    \"\"\"\n",
    "    section_names = ['case_facts', 'issues', 'arguments', 'decision']\n",
    "    section_index = section_names.index(section_type)\n",
    "    \n",
    "    # Generate query embedding\n",
    "    query_embedding = model.encode(query, convert_to_numpy=True)\n",
    "    \n",
    "    # Create full embedding vector with query in the right position\n",
    "    dimension = model.get_sentence_embedding_dimension()\n",
    "    full_embedding = np.zeros(dimension * 4)\n",
    "    full_embedding[section_index * dimension:(section_index + 1) * dimension] = query_embedding\n",
    "    \n",
    "    # Normalize\n",
    "    full_embedding = full_embedding / np.linalg.norm(full_embedding)\n",
    "    full_embedding = full_embedding.reshape(1, -1)\n",
    "    \n",
    "    # Search\n",
    "    distances, indices = index.search(full_embedding.astype('float32'), top_k)\n",
    "    \n",
    "    results = []\n",
    "    for idx, distance in zip(indices[0], distances[0]):\n",
    "        if idx < len(metadata):\n",
    "            meta = metadata[idx]\n",
    "            section_text = meta['sections'][section_type]\n",
    "            text_snippet = section_text[:200] + \"...\" if len(section_text) > 200 else section_text\n",
    "            results.append((meta['filename'], float(distance), text_snippet))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def search_all_sections(query, index, metadata, model, top_k=5):\n",
    "    \"\"\"\n",
    "    Search across all sections equally.\n",
    "    \n",
    "    Args:\n",
    "        query: Query text\n",
    "        index: FAISS index\n",
    "        metadata: List of metadata dictionaries\n",
    "        model: SentenceTransformer model\n",
    "        top_k: Number of top results to return\n",
    "        \n",
    "    Returns:\n",
    "        List of tuples: (filename, similarity_score, sections_dict)\n",
    "    \"\"\"\n",
    "    # Generate query embedding for all sections\n",
    "    query_embedding = model.encode(query, convert_to_numpy=True)\n",
    "    \n",
    "    # Repeat query embedding 4 times (for all sections)\n",
    "    full_embedding = np.tile(query_embedding, 4)\n",
    "    \n",
    "    # Normalize\n",
    "    full_embedding = full_embedding / np.linalg.norm(full_embedding)\n",
    "    full_embedding = full_embedding.reshape(1, -1)\n",
    "    \n",
    "    # Search\n",
    "    distances, indices = index.search(full_embedding.astype('float32'), top_k)\n",
    "    \n",
    "    results = []\n",
    "    for idx, distance in zip(indices[0], distances[0]):\n",
    "        if idx < len(metadata):\n",
    "            meta = metadata[idx]\n",
    "            results.append((meta['filename'], float(distance), meta['sections']))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    FOLDER_PATH = \"summary\"  # Change this to your folder path\n",
    "    OUTPUT_DIR = \"./\"\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Read and parse judgment files\n",
    "        print(\"=\" * 60)\n",
    "        print(\"STEP 1: Reading and parsing judgment files\")\n",
    "        print(\"=\" * 60)\n",
    "        judgments = read_judgment_files(FOLDER_PATH)\n",
    "        print(f\"\\nTotal files read: {len(judgments)}\")\n",
    "        \n",
    "        if not judgments:\n",
    "            print(\"No judgment files found. Please check the folder path.\")\n",
    "            exit(1)\n",
    "        \n",
    "        # Step 2: Create section embeddings (4 sections per judgment as single block)\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"STEP 2: Creating section embeddings\")\n",
    "        print(\"=\" * 60)\n",
    "        embeddings, metadata_list, model = create_section_embeddings(judgments)\n",
    "        print(f\"\\nGenerated {len(embeddings)} judgment vectors\")\n",
    "        print(f\"Each vector contains 4 section embeddings concatenated\")\n",
    "        \n",
    "        # Step 3: Create FAISS index\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"STEP 3: Creating FAISS index\")\n",
    "        print(\"=\" * 60)\n",
    "        index = create_faiss_index(embeddings)\n",
    "        \n",
    "        # Step 4: Save index and metadata (JSON format)\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"STEP 4: Saving index and metadata\")\n",
    "        print(\"=\" * 60)\n",
    "        save_index_and_metadata(index, metadata_list, OUTPUT_DIR)\n",
    "        \n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08e4cd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created and saved 511 IPC sections:\n",
      "Index: ipc.index\n",
      "Metadata: ipc.json\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# --- Configuration (Update as needed) ---\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "CSV_FILE = \"ipc_formatted_clean.csv\"\n",
    "INDEX_FILE = \"ipc.index\"\n",
    "METADATA_FILE = \"ipc.json\"\n",
    "\n",
    "# --- 1. Load and Prepare Data ---\n",
    "\n",
    "# Attempt to load the file using a permissive encoding to avoid Unicode errors\n",
    "ipc = pd.read_csv(CSV_FILE, encoding='cp1252')\n",
    "\n",
    "# Fix the 'AttributeError: 'float' object has no attribute 'strip'' by filling NaN values.\n",
    "# This is crucial for robust string processing.\n",
    "columns_to_clean = ['Section Code', 'Description', 'Punishment/Consequence']\n",
    "for col in columns_to_clean:\n",
    "    if col in ipc.columns:\n",
    "        ipc[col] = ipc[col].fillna('').astype(str)\n",
    "\n",
    "# --- 2. Format Data for Embeddings (The \"final_text\" creation logic) ---\n",
    "\n",
    "def format_section(row):\n",
    "    \"\"\"Formats a single row into a coherent document block for the Sentence Transformer.\"\"\"\n",
    "    section_code = row[\"Section Code\"].strip()\n",
    "    description = row[\"Description\"].strip()\n",
    "    punishment = row[\"Punishment/Consequence\"].strip()\n",
    "    \n",
    "    # Create the clean, dense text block for semantic search\n",
    "    return f\"\"\"\n",
    "SECTION: {section_code}\n",
    "DESCRIPTION: {description}\n",
    "PUNISHMENT: {punishment}\n",
    "\"\"\"\n",
    "\n",
    "# Apply the function to the DataFrame to get a list of formatted documents\n",
    "formatted_series = ipc.apply(format_section, axis=1)\n",
    "\n",
    "# Documents for Embedding (the core text) and Metadata (for lookup)\n",
    "documents = formatted_series.tolist()\n",
    "metadata = ipc.rename(columns={'Section Code': 'section_no', 'Punishment/Consequence': 'punishment_raw'}).to_dict('records')\n",
    "\n",
    "# --- 3. Create Embeddings and FAISS Index ---\n",
    "\n",
    "# Load embedding model\n",
    "embedder = SentenceTransformer(EMBEDDING_MODEL)\n",
    "\n",
    "# Generate embeddings for all documents\n",
    "embeddings = embedder.encode(documents)\n",
    "\n",
    "# Convert to a float32 NumPy array, as required by FAISS\n",
    "embeddings_np = np.array(embeddings).astype(\"float32\")\n",
    "dimension = embeddings_np.shape[1]\n",
    "\n",
    "# Create a FAISS Index (IndexFlatL2 uses Euclidean distance for search)\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings_np)\n",
    "\n",
    "# --- 4. Save Index and Metadata ---\n",
    "\n",
    "# Save the FAISS index\n",
    "faiss.write_index(index, INDEX_FILE)\n",
    "\n",
    "# Save the metadata (original data) mapped by its index position\n",
    "with open(METADATA_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"Successfully created and saved {len(documents)} IPC sections:\")\n",
    "print(f\"Index: {INDEX_FILE}\")\n",
    "print(f\"Metadata: {METADATA_FILE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
