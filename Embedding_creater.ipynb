{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f702c79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc02a0d5923d4fdb80d82bc99ebfd190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/341 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\ml\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\nihca\\.cache\\huggingface\\hub\\models--bhavyagiri--InLegal-Sbert. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35cd7fdf265b47798dfe6bd954ff865a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62932016220347d89891ae5b85f64155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2829d476d3eb44c785690f9c9f6b4dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac0729fb18364290a23f4d8143db57f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92e85fe9ae36423e94bdb0f60f45f6a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cebf3be9601a4b698b5d36e4a33bfa51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d5ad858058d48f3bc6d95a6f6335a39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "247d86eb585044ca8bf22946c05b7819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e850ebcce4a4ad89250d3ecafe2bb10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c7f88ee063948c6b10f2d88c2888983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "934db4988472475487168214ecc47e7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6630e4462d3141de9c61a7205984820a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/114 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fea93c577594970a3e2c37b45b19034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2_Dense/pytorch_model.bin:   0%|          | 0.00/2.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 933 documents. JSON and FAISS index saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ======= CONFIG =======\n",
    "SUMMARY_FOLDER = \"summary\"\n",
    "JSON_OUTPUT = \"judgments.json\"\n",
    "FAISS_INDEX = \"judgments.index\"\n",
    "EMBEDDING_MODEL = \"bhavyagiri/InLegal-Sbert\"\n",
    "# ======================\n",
    "\n",
    "# Load embedding model\n",
    "model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "\n",
    "# Regex patterns for flexible section detection\n",
    "section_patterns = {\n",
    "    \"case_facts\": re.compile(r\"(case\\s*facts)\", re.IGNORECASE),\n",
    "    \"issues\": re.compile(r\"(issues?)\", re.IGNORECASE),\n",
    "    \"arguments\": re.compile(r\"(arguments?|reasoning)\", re.IGNORECASE),\n",
    "    \"decision\": re.compile(r\"(decision|holding|judgment)\", re.IGNORECASE)\n",
    "}\n",
    "\n",
    "def extract_sections_fuzzy(text):\n",
    "    sections = {k: \"\" for k in section_patterns.keys()}\n",
    "    current_key = None\n",
    "\n",
    "    for line in text.splitlines():\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # Check if line matches any section heading\n",
    "        matched = False\n",
    "        for key, pattern in section_patterns.items():\n",
    "            if pattern.search(line):  # If heading found\n",
    "                current_key = key\n",
    "                matched = True\n",
    "                break\n",
    "        \n",
    "        # If line is not heading, add to current section\n",
    "        if current_key and not matched:\n",
    "            sections[current_key] += line + \" \"\n",
    "\n",
    "    return sections\n",
    "\n",
    "# Process all summaries in folder\n",
    "documents = []\n",
    "for file_name in os.listdir(SUMMARY_FOLDER):\n",
    "    if not file_name.endswith(\".txt\"):\n",
    "        continue\n",
    "\n",
    "    file_path = os.path.join(SUMMARY_FOLDER, file_name)\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    sections = extract_sections_fuzzy(text)\n",
    "    doc_id = os.path.splitext(file_name)[0]\n",
    "\n",
    "    doc = {\n",
    "        \"id\": doc_id,\n",
    "        \"case_facts\": sections[\"case_facts\"].strip(),\n",
    "        \"issues\": sections[\"issues\"].strip(),\n",
    "        \"arguments\": sections[\"arguments\"].strip(),\n",
    "        \"decision\": sections[\"decision\"].strip()\n",
    "    }\n",
    "    documents.append(doc)\n",
    "\n",
    "# Save as JSON\n",
    "with open(JSON_OUTPUT, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(documents, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Build FAISS index\n",
    "texts = [doc[\"case_facts\"] + \" \" + doc[\"issues\"] + \" \" + doc[\"arguments\"] + \" \" + doc[\"decision\"] for doc in documents]\n",
    "embeddings = model.encode(texts, convert_to_numpy=True)\n",
    "\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n",
    "\n",
    "faiss.write_index(index, FAISS_INDEX)\n",
    "\n",
    "print(f\"Processed {len(documents)} documents. JSON and FAISS index saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08e4cd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created and saved 511 IPC sections:\n",
      "Index: ipc.index\n",
      "Metadata: ipc.json\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# --- Configuration (Update as needed) ---\n",
    "EMBEDDING_MODEL = \"bhavyagiri/InLegal-Sbert\"\n",
    "CSV_FILE = \"ipc_formatted_clean.csv\"\n",
    "INDEX_FILE = \"ipc.index\"\n",
    "METADATA_FILE = \"ipc.json\"\n",
    "\n",
    "# --- 1. Load and Prepare Data ---\n",
    "\n",
    "# Attempt to load the file using a permissive encoding to avoid Unicode errors\n",
    "ipc = pd.read_csv(CSV_FILE, encoding='cp1252')\n",
    "\n",
    "# Fix the 'AttributeError: 'float' object has no attribute 'strip'' by filling NaN values.\n",
    "# This is crucial for robust string processing.\n",
    "columns_to_clean = ['Section Code', 'Description', 'Punishment/Consequence']\n",
    "for col in columns_to_clean:\n",
    "    if col in ipc.columns:\n",
    "        ipc[col] = ipc[col].fillna('').astype(str)\n",
    "\n",
    "# --- 2. Format Data for Embeddings (The \"final_text\" creation logic) ---\n",
    "\n",
    "def format_section(row):\n",
    "    \"\"\"Formats a single row into a coherent document block for the Sentence Transformer.\"\"\"\n",
    "    section_code = row[\"Section Code\"].strip()\n",
    "    description = row[\"Description\"].strip()\n",
    "    punishment = row[\"Punishment/Consequence\"].strip()\n",
    "    \n",
    "    # Create the clean, dense text block for semantic search\n",
    "    return f\"\"\"\n",
    "SECTION: {section_code}\n",
    "DESCRIPTION: {description}\n",
    "PUNISHMENT: {punishment}\n",
    "\"\"\"\n",
    "\n",
    "# Apply the function to the DataFrame to get a list of formatted documents\n",
    "formatted_series = ipc.apply(format_section, axis=1)\n",
    "\n",
    "# Documents for Embedding (the core text) and Metadata (for lookup)\n",
    "documents = formatted_series.tolist()\n",
    "metadata = ipc.rename(columns={'Section Code': 'section_no', 'Punishment/Consequence': 'punishment_raw'}).to_dict('records')\n",
    "\n",
    "# --- 3. Create Embeddings and FAISS Index ---\n",
    "\n",
    "# Load embedding model\n",
    "embedder = SentenceTransformer(EMBEDDING_MODEL)\n",
    "\n",
    "# Generate embeddings for all documents\n",
    "embeddings = embedder.encode(documents)\n",
    "\n",
    "# Convert to a float32 NumPy array, as required by FAISS\n",
    "embeddings_np = np.array(embeddings).astype(\"float32\")\n",
    "dimension = embeddings_np.shape[1]\n",
    "\n",
    "# Create a FAISS Index (IndexFlatL2 uses Euclidean distance for search)\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings_np)\n",
    "\n",
    "# --- 4. Save Index and Metadata ---\n",
    "\n",
    "# Save the FAISS index\n",
    "faiss.write_index(index, INDEX_FILE)\n",
    "\n",
    "# Save the metadata (original data) mapped by its index position\n",
    "with open(METADATA_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"Successfully created and saved {len(documents)} IPC sections:\")\n",
    "print(f\"Index: {INDEX_FILE}\")\n",
    "print(f\"Metadata: {METADATA_FILE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
