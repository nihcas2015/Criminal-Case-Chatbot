{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "349d4443",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.chains import ConversationChain\n",
    "from typing import Optional, List, Any\n",
    "import faiss\n",
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30d4ba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer(\"bhavyagiri/InLegal-Sbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db008976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_index_and_meta(index_path, meta_path):\n",
    "    index = faiss.read_index(index_path)\n",
    "    with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        meta = json.load(f)\n",
    "    return index, meta\n",
    "\n",
    "def search_faiss(query, index, meta, top_k=2):\n",
    "    query_vec = embedder.encode([query])\n",
    "    query_vec = np.array(query_vec).astype(\"float32\")\n",
    "    D, I = index.search(query_vec, top_k)\n",
    "    results = []\n",
    "    for idx in I[0]:\n",
    "        if idx < len(meta):\n",
    "            results.append(meta[idx])\n",
    "    return results\n",
    "class ollama(LLM):\n",
    "    model_name: str = \"mistral\"\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"ollama\"\n",
    "        \n",
    "    def _call(self,prompt: str, stop: Optional[List[str]] = None, **kwargs: Any):\n",
    "        response = requests.post(\n",
    "            \"http://127.0.0.1:11434/api/generate\",\n",
    "            json={\"model\": self.model_name, \"prompt\": prompt, \"stream\": False}\n",
    "        )\n",
    "        return response.json()[\"response\"]\n",
    "\n",
    "def get_top_judgment_text(query):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
    "    url = f\"https://indiankanoon.org/search/?formInput={query}&type=judgement\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    results = soup.find_all(\"div\", class_=\"result_title\")\n",
    "    for result in results:\n",
    "        link_tag = result.find('a')\n",
    "        if not link_tag:\n",
    "            continue\n",
    "        link = link_tag['href']\n",
    "        full_link = \"https://indiankanoon.org\" + link\n",
    "        judgment_page = requests.get(full_link, headers=headers)\n",
    "        judgment_soup = BeautifulSoup(judgment_page.text, 'html.parser')\n",
    "        judgment_div = judgment_soup.find(\"div\", {\"id\": \"judgment\"})\n",
    "        if judgment_div:\n",
    "            judgment_text = judgment_div.get_text(separator=\"\\n\", strip=True)\n",
    "        else:\n",
    "            judgment_text = judgment_soup.get_text(separator=\"\\n\", strip=True)\n",
    "        return judgment_text\n",
    "    return None\n",
    "def summarize_long_text(text, tokenizer, model, chunk_size=1024, overlap=100):\n",
    "    # Split into words for rough chunking\n",
    "    words = text.split()\n",
    "    summaries = []\n",
    "\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = \" \".join(words[i:i + chunk_size])\n",
    "        inputs = tokenizer(chunk, return_tensors=\"pt\", max_length=chunk_size, truncation=True)\n",
    "        summary_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_length=200,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        summaries.append(tokenizer.decode(summary_ids[0], skip_special_tokens=True))\n",
    "\n",
    "    # Optionally combine and summarize again for final condensation\n",
    "    combined_summary = \" \".join(summaries)\n",
    "    final_inputs = tokenizer(combined_summary, return_tensors=\"pt\", truncation=True)\n",
    "    final_summary_ids = model.generate(\n",
    "        **final_inputs,\n",
    "        max_length=300,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    return tokenizer.decode(final_summary_ids[0], skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d924f68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipc_index, ipc_meta = load_index_and_meta(\"ipc.index\", \"ipc.json\")\n",
    "\n",
    "model_dir = \"./legal_summarizer_model\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d2f7be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom=PromptTemplate(input_variables=[\"history\",\"input\"],template=\"\"\" You are an legal Chatbot assistant specializing in India Penal Code.\n",
    "                      \n",
    "                      Previous conversations:{history}\n",
    "                      \n",
    "                      Current question and also similar judgements :{input}\n",
    "                      \n",
    "                      Read user query carefully and Speak to them like an chatbot.\n",
    "                      Utilize judgements given and also previous conversations for your knowledge.\n",
    "                      DO NOT REPEAT ABOUT THE SAME JUDGEMENT FOR EVERY REPLY \n",
    "                      BE CONCISE BUT ALSO SPEAK WELL.\n",
    "                      DO NOT HALLUCINATE WITH YOUR LEGAL KNOWLEDGE.\n",
    "                      CONSIDER YOURSELF WELL TRAINED LEGAL CHATBOT.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbc7d832",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ollama()\n",
    "memory=ConversationBufferMemory()\n",
    "conversion = ConversationChain(llm=llm,memory=memory,prompt=custom,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "875ecf6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öñÔ∏è IPC + Judgement Chatbot (type 'exit' to quit)\n",
      "\n",
      "Bot:  I'm sorry to hear about your situation. Under the Indian Penal Code (IPC), the act of killing someone is punishable under various sections, depending on the circumstances involved. However, the section that specifically pertains to organized crime and murder, as in your friend's case, is Section 310 - Thug.\n",
      "\n",
      "According to this section, if your friend has been habitually associated with you or others for the purpose of committing robbery or child-stealing by means of or accompanied with murder, then they could be considered a \"thug.\" The punishment for this offense includes imprisonment for life or even death.\n",
      "\n",
      "It's important to note that I am just a chatbot and not a lawyer. Therefore, I strongly recommend seeking legal advice from a professional to understand the specific implications of this situation better. This advice could help you determine the best course of action based on the evidence available and the unique circumstances of your case.\n",
      "Bot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "print(\"‚öñÔ∏è IPC + Judgement Chatbot (type 'exit' to quit)\\n\")\n",
    "\n",
    "user_input = input(\"You: \").strip()\n",
    "\n",
    "\n",
    "ipc_results = search_faiss(user_input, ipc_index, ipc_meta)\n",
    "context = f\"User query : {user_input} \\n \"\n",
    "for item in ipc_results:\n",
    "    context+=f\"\\nIPC \\n {item['section_no']} \\n Description:{item['Description']} \\n Punishments : {item['punishment_raw']} \\n Judgement Summary: \\n\"\n",
    "    if 'section_no' in item and ':-' in item['section_no']:\n",
    "        item['section_no'] = item['section_no'].split(':-', 1)[1].strip()\n",
    "        context += summarize_long_text(get_top_judgment_text(item[\"section_no\"]), tokenizer, model)\n",
    "\n",
    "\n",
    "print(\"Bot:\",conversion.predict(input=context))\n",
    "while True:\n",
    "    user_input = input(\"You: \").strip()\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Bot: Goodbye!\")\n",
    "        break\n",
    "\n",
    "    answer = conversion.predict(input=user_input)\n",
    "\n",
    "    print(f\"\\nBot: {answer}\\n\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59e9847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43ef36d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User query : my friend killed my brother \n",
      " \n",
      "IPC \n",
      " Section310:- Thug \n",
      " Description:Whoever, at any time after the passing of this Act, shall have been habitually associated with any other or others for the purpose of committing robbery or child-stealing by means of or accompanied with murder, is a thug. \n",
      " Punishments :  \n",
      " Judgement Summary: \n",
      "The petitioner who was detained under the Preventive Detention Act (Act IV of 1950) applied under article 32 of the Constitution for a writ of habeas corpus and for his release from detention on the ground that the said Act contravened the provisions of articles 13 19 21 and 22 of the Constitu tion and was consequently ultra rites and that his detention was therefore illegal: Held per KANIA C.J. PATANJALI SASTRI MUKHERJEA and DAS JJ. (FAZL ALI and MAHAJAN JJ. dissentinq) that the preventive Detention Act 1950 with the exception of Sec. 14 thereof did not contravene any of the Articles of the Constitution and even though Sec. 14 was ultra rites inas much as it contravened the provisions of article 9.9 (5) of the Constitution as this section was severable from the remaining sections of the Act the invalidity of Sec. 14 did not affect the validity of the Act as a whole and the detention of the petitioner was not illegal. FAZL ALl and MAHAJAN JJ. Section\n",
      "IPC \n",
      " Section132:- Abetment of mutiny, if mutiny is committed in consequence thereof \n",
      " Description:Whoever abets the committing of mutiny by an officer, soldier, sailor or airman, in the Army, Navy or Air Force of the Government of India, shall, if mutiny be committed in consequence of that abetment, be punished with \n",
      " Punishments : death or with imprisonment for life, or imprisonment of either description for a term which may extend to ten years, and shall also be liable to fine. \n",
      " Judgement Summary: \n",
      "Section 132 of the Indian Penal Code read with section 132 of the Code of Criminal Procedure provides that if a person abetments the committing of a mutiny in consequence of that abetment he shall be punished with death or with imprisonment for life or with imprisonment of either description for a term which may extend to ten years and shall also be liable to fine. Section 132 of the Code of Criminal Procedure provides that if a person abetments the committing of a mutiny in consequence of that abetment he shall be punished with death or with imprisonment for life or with imprisonment of either description for a term which may extend to ten years and shall also be liable to fine. Section 132 of the Indian Penal Code read with section 132 of the Code of Criminal Procedure provides that if a person abetments the committing of a mutiny in consequence of that abetment he shall be punished with death or with imprisonment for life or with imprisonment of either description for a term which may extend to ten years and shall also be liable to fine. Section 132 of the Indian\n"
     ]
    }
   ],
   "source": [
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369bec61",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = tokenizer(get_top_judgment_text(item[\"section_no\"]), return_tensors=\"pt\", max_length=1024, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df7a7d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_ids = model.generate(\n",
    "    **summary,\n",
    "    max_length=200,   \n",
    "    num_beams=4,\n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d3e12da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The first respondent B purchased a Touzi in 24 Parganas Collectorate at a revenue sale held on 9th January 1942. As such purchaser he acquired under section 37 of the Bengal Revenue Sales Act 1859 the right \"to avoid and annul all under tenures and forthwith to eject all under tenants\" with certain exceptions which are not material here. In exercise of that right he gave notices of ejectment and brought a suit in 1946 to evict certain under tenants including the second respondent herein and to recover possession of the lands. The suit was decreed against the second respondent who preferred an appeal to the District Judge 24 Parganas contending that his under tenure came within one of the exceptions referred to in section 37. When the appeal was pending the Bill which was later passed as the West Bengal Revenue Sales (West Bengal Amendment) Act 1950 was introduced in the West Bengal'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afab6501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Arun Singh vs State Of U.P. on 10 February, 2020\\nShare Link\\nMobile View\\nFree features\\nPremium features\\nCase removal\\nView Complete document\\nArun Singh vs State Of U.P. on 10 February, 2020\\nShowing the contexts in which\\nCohabitation caused by a man deceitfully inducing a belief of lawful marriage\\nappears in the document\\nChange context size\\nCurrent\\n‚Äú\\nCohabit\\na\\ntion\\ncaused\\nby\\na\\nman\\ndeceit\\nfully\\ninducing\\na\\nbelief\\nof\\nlaw\\nful\\nmarriage\\n. - Every\\nman\\nwho by\\ndeceit\\ncauses\\na\\nny\\n         wo\\nman\\nwho is not\\nlaw\\nfully m\\na\\nrried to him to believe th\\na\\nt she is\\nlaw\\nfully m\\na\\nrried to him\\na\\nnd to\\ncohabit\\nor h\\na\\nve sexu\\na\\nl\\n         intercourse with him in th\\na\\nt\\nbelief\\n, sh\\na\\nll be punished with\\n         imprisonment of either description for\\na\\nterm which m\\nay\\nextend\\n         to ten ye\\na\\nrs,\\na\\nnd sh\\na\\nll\\na\\nlso be li\\na\\nble to fine.‚Äù\\n18.\\nA\\npl\\na\\nin re\\na\\nding of the Section goes to show th\\na\\nt in order to constitute\\na\\nn offence under this Section, it h\\nas\\nto be demonstr\\nated\\nth\\na\\nt\\na\\nman\\nh\\nas\\ndeceit\\nfully\\ncaused\\na\\nny wo\\nman\\n, who is not\\nlaw\\nfully m\\na\\nrried to him, to believe\\n\\n\\nth\\na\\nt she is\\nlaw\\nfully m\\na\\nrried wife\\na\\nnd thereby co-h\\na\\nbit with him. In other\\n\\nwords, the\\na\\nccused must\\ninduce\\na\\nwo\\nman\\n, not\\nlaw\\nfully m\\na\\nrried to him, to\\n\\nbelieve th\\na\\nt she is m\\na\\nrried to him\\na\\nnd\\nas\\na\\nresult of such mis-represent\\na\\ntion,\\n\\nwo\\nman\\nshould believe th\\na\\nt she w\\nas\\nlaw\\nfully m\\na\\nrried to the\\nman\\na\\nnd thus there\\n\\nshould be co-h\\na\\nbit\\na\\ntion or sexu\\na\\nl intercourse.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_judgment_text(item[\"section_no\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed13a521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Arun Singh vs State Of U.P. on 10 February, 2020\\nShare Link\\nMobile View\\nFree features\\nPremium features\\nCase removal\\nView Complete document\\nArun Singh vs State Of U.P. on 10 February, 2020\\nShowing the contexts in which\\nCohabitation caused by a man deceitfully inducing a belief of lawful marriage\\nappears in the document\\nChange context size\\nCurrent\\n‚Äú\\nCohabit\\na\\ntion\\ncaused\\nby\\na\\nman\\ndeceit\\nfully\\ninducing\\na\\nbelief\\nof\\nlaw\\nful\\nmarriage\\n. - Every\\nman\\nwho by\\ndeceit\\ncauses\\na\\nny\\n         wo\\nman\\nwho is not\\nlaw\\nfully m\\na\\nrried to him to believe th\\na\\nt she is\\nlaw\\nfully m\\na\\nrried to him\\na\\nnd to\\ncohabit\\nor h\\na\\nve sexu\\na\\nl\\n         intercourse with him in th\\na\\nt\\nbelief\\n, sh\\na\\nll be punished with\\n         imprisonment of either description for\\na\\nterm which m\\nay\\nextend\\n         to ten ye\\na\\nrs,\\na\\nnd sh\\na\\nll\\na\\nlso be li\\na\\nble to fine.‚Äù\\n18.\\nA\\npl\\na\\nin re\\na\\nding of the Section goes to show th\\na\\nt in order to constitute\\na\\nn offence under this Section, it h\\nas\\nto be demonstr\\nated\\nth\\na\\nt\\na\\nman\\nh\\nas\\ndeceit\\nfully\\ncaused\\na\\nny wo\\nman\\n, who is not\\nlaw\\nfully m\\na\\nrried to him, to believe\\n\\n\\nth\\na\\nt she is\\nlaw\\nfully m\\na\\nrried wife\\na\\nnd thereby co-h\\na\\nbit with him. In other\\n\\nwords, the\\na\\nccused must\\ninduce\\na\\nwo\\nman\\n, not\\nlaw\\nfully m\\na\\nrried to him, to\\n\\nbelieve th\\na\\nt she is m\\na\\nrried to him\\na\\nnd\\nas\\na\\nresult of such mis-represent\\na\\ntion,\\n\\nwo\\nman\\nshould believe th\\na\\nt she w\\nas\\nlaw\\nfully m\\na\\nrried to the\\nman\\na\\nnd thus there\\n\\nshould be co-h\\na\\nbit\\na\\ntion or sexu\\na\\nl intercourse.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_judgment_text(item[\"section_no\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8260dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input(\"You: \").strip()\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Bot: Goodbye!\")\n",
    "        break\n",
    "\n",
    "    \n",
    "\n",
    "    answer = conversion.predict(input=user_input)\n",
    "\n",
    "    print(f\"\\nBot: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dde3bbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ready with 10 test queries\n",
      "‚úÖ Using evaluation model: bhavyagiri/InLegal-Sbert (your existing embedder)\n"
     ]
    }
   ],
   "source": [
    "# Test queries for evaluation\n",
    "test_queries = [\n",
    "    \"What is the punishment for murder?\",\n",
    "    \"Explain Section 302 IPC\",\n",
    "    \"What are the differences between theft and robbery?\",\n",
    "    \"Punishment for dowry death\",\n",
    "    \"What is culpable homicide?\",\n",
    "    \"Explain Section 420 IPC about cheating\",\n",
    "    \"What is the punishment for kidnapping?\",\n",
    "    \"Define Section 376 IPC\",\n",
    "    \"What are the provisions for defamation?\",\n",
    "    \"Explain attempt to murder under IPC\"\n",
    "]\n",
    "\n",
    "# Use the SAME model you're already using (no need to load a new one)\n",
    "eval_model = embedder  # Reuse your existing InLegal-Sbert model\n",
    "\n",
    "print(f\"‚úÖ Ready with {len(test_queries)} test queries\")\n",
    "print(f\"‚úÖ Using evaluation model: bhavyagiri/InLegal-Sbert (your existing embedder)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19db7f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RETRIEVAL CONSISTENCY\n",
      "============================================================\n",
      "\n",
      "Query: 'What is the punishment for murder?'\n",
      "üìä IPC Retrieval Consistent: ‚úÖ YES\n",
      "üìä Judgement Retrieval Consistent: ‚úÖ YES\n",
      "   Retrieved: ('Section302:- Punishment for murder', 'Section507:- Criminal intimidation by an anonymous communication')\n"
     ]
    }
   ],
   "source": [
    "# EVALUATION 1: RETRIEVAL CONSISTENCY\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RETRIEVAL CONSISTENCY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_query = \"What is the punishment for murder?\"\n",
    "num_runs = 5\n",
    "\n",
    "ipc_retrieved = []\n",
    "judg_retrieved = []\n",
    "\n",
    "for run in range(num_runs):\n",
    "    ipc_results = search_faiss(test_query, ipc_index, ipc_meta, top_k=2)\n",
    "    judg_results = search_faiss(test_query, judg_index, judg_meta, top_k=2)\n",
    "    \n",
    "    ipc_ids = tuple([r['section_no'] for r in ipc_results])\n",
    "    judg_ids = tuple([r['id'] for r in judg_results])\n",
    "    \n",
    "    ipc_retrieved.append(ipc_ids)\n",
    "    judg_retrieved.append(judg_ids)\n",
    "\n",
    "ipc_consistent = len(set(ipc_retrieved)) == 1\n",
    "judg_consistent = len(set(judg_retrieved)) == 1\n",
    "\n",
    "print(f\"\\nQuery: '{test_query}'\")\n",
    "print(f\"üìä IPC Retrieval Consistent: {'‚úÖ YES' if ipc_consistent else '‚ùå NO'}\")\n",
    "print(f\"üìä Judgement Retrieval Consistent: {'‚úÖ YES' if judg_consistent else '‚ùå NO'}\")\n",
    "print(f\"   Retrieved: {ipc_retrieved[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54b04f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ANSWER CONSISTENCY\n",
      "============================================================\n",
      "\n",
      "üìä Consistency Score: 0.970\n",
      "   Min Similarity: 0.964\n",
      "   Max Similarity: 0.976\n",
      "   ‚úÖ HIGH consistency\n"
     ]
    }
   ],
   "source": [
    "# EVALUATION 2: ANSWER CONSISTENCY\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANSWER CONSISTENCY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_query = \"What is the punishment for murder?\"\n",
    "num_samples = 3\n",
    "\n",
    "answers = []\n",
    "for i in range(num_samples):\n",
    "    ipc_results = search_faiss(test_query, ipc_index, ipc_meta, top_k=2)\n",
    "    \n",
    "    context = \"Relevant IPC Sections:\\n\"\n",
    "    for sec in ipc_results:\n",
    "        context += f\"- Section {sec['section_no']}: {sec['Description']} (Punishment: {sec['punishment_raw']})\\n\"\n",
    "    \n",
    "    prompt = f\"User Query: {test_query}\\n\\n{context}\\n\\nBased on the above IPC sections, Name the sections relating to the user query and also list possible punishments.\"\n",
    "    answer = llm._call(prompt)\n",
    "    answers.append(answer)\n",
    "\n",
    "embeddings = eval_model.encode(answers, convert_to_tensor=True)\n",
    "similarities = []\n",
    "\n",
    "for i in range(len(answers)):\n",
    "    for j in range(i+1, len(answers)):\n",
    "        sim = util.cos_sim(embeddings[i], embeddings[j]).item()\n",
    "        similarities.append(sim)\n",
    "\n",
    "consistency_score = np.mean(similarities)\n",
    "\n",
    "print(f\"\\nüìä Consistency Score: {consistency_score:.3f}\")\n",
    "print(f\"   Min Similarity: {np.min(similarities):.3f}\")\n",
    "print(f\"   Max Similarity: {np.max(similarities):.3f}\")\n",
    "\n",
    "if consistency_score > 0.8:\n",
    "    print(f\"   ‚úÖ HIGH consistency\")\n",
    "elif consistency_score > 0.6:\n",
    "    print(f\"   ‚ö†Ô∏è MODERATE consistency\")\n",
    "else:\n",
    "    print(f\"   ‚ùå LOW consistency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2eeb75c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CONTEXT RELEVANCE\n",
      "============================================================\n",
      "Query: What is the punishment for murder?... ‚Üí Score: 0.659\n",
      "Query: Explain Section 302 IPC... ‚Üí Score: 0.673\n",
      "Query: What are the differences between theft and robbery... ‚Üí Score: 0.743\n",
      "Query: Punishment for dowry death... ‚Üí Score: 0.698\n",
      "Query: What is culpable homicide?... ‚Üí Score: 0.680\n",
      "\n",
      "üìä Mean Context Relevance: 0.691\n",
      "   Range: [0.659, 0.743]\n"
     ]
    }
   ],
   "source": [
    "# EVALUATION 3: CONTEXT RELEVANCE\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONTEXT RELEVANCE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "relevance_scores = []\n",
    "\n",
    "for query in test_queries[:5]:\n",
    "    ipc_results = search_faiss(query, ipc_index, ipc_meta, top_k=2)\n",
    "    \n",
    "    context = \"Relevant IPC Sections:\\n\"\n",
    "    for sec in ipc_results:\n",
    "        context += f\"- Section {sec['section_no']}: {sec['Description']} (Punishment: {sec['punishment_raw']})\\n\"\n",
    "    \n",
    "    query_emb = eval_model.encode(query, convert_to_tensor=True)\n",
    "    context_emb = eval_model.encode(context, convert_to_tensor=True)\n",
    "    similarity = util.cos_sim(query_emb, context_emb).item()\n",
    "    \n",
    "    relevance_scores.append(similarity)\n",
    "    print(f\"Query: {query[:50]}... ‚Üí Score: {similarity:.3f}\")\n",
    "\n",
    "print(f\"\\nüìä Mean Context Relevance: {np.mean(relevance_scores):.3f}\")\n",
    "print(f\"   Range: [{np.min(relevance_scores):.3f}, {np.max(relevance_scores):.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7019eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ANSWER RELEVANCE\n",
      "============================================================\n",
      "Query: What is the punishment for murder?... ‚Üí Score: 0.671\n",
      "Query: Explain Section 302 IPC... ‚Üí Score: 0.619\n",
      "Query: What are the differences between theft and robbery... ‚Üí Score: 0.753\n",
      "Query: Punishment for dowry death... ‚Üí Score: 0.703\n",
      "Query: What is culpable homicide?... ‚Üí Score: 0.693\n",
      "\n",
      "üìä Mean Answer Relevance: 0.688\n",
      "   Range: [0.619, 0.753]\n"
     ]
    }
   ],
   "source": [
    "# EVALUATION 4: ANSWER RELEVANCE\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANSWER RELEVANCE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "relevance_scores = []\n",
    "\n",
    "for query in test_queries[:5]:\n",
    "    ipc_results = search_faiss(query, ipc_index, ipc_meta, top_k=2)\n",
    "    \n",
    "    context = \"Relevant IPC Sections:\\n\"\n",
    "    for sec in ipc_results:\n",
    "        context += f\"- Section {sec['section_no']}: {sec['Description']} (Punishment: {sec['punishment_raw']})\\n\"\n",
    "    \n",
    "    prompt = f\"User Query: {query}\\n\\n{context}\\n\\nBased on the above IPC sections, Name the sections relating to the user query and also list possible punishments.\"\n",
    "    answer = llm._call(prompt)\n",
    "    \n",
    "    query_emb = eval_model.encode(query, convert_to_tensor=True)\n",
    "    answer_emb = eval_model.encode(answer, convert_to_tensor=True)\n",
    "    similarity = util.cos_sim(query_emb, answer_emb).item()\n",
    "    \n",
    "    relevance_scores.append(similarity)\n",
    "    print(f\"Query: {query[:50]}... ‚Üí Score: {similarity:.3f}\")\n",
    "\n",
    "print(f\"\\nüìä Mean Answer Relevance: {np.mean(relevance_scores):.3f}\")\n",
    "print(f\"   Range: [{np.min(relevance_scores):.3f}, {np.max(relevance_scores):.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "04d3b6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FAITHFULNESS (Answer grounded in Context)\n",
      "============================================================\n",
      "Query: What is the punishment for murder?... ‚Üí Score: 0.932\n",
      "Query: Explain Section 302 IPC... ‚Üí Score: 0.926\n",
      "Query: What are the differences between theft and robbery... ‚Üí Score: 0.945\n",
      "Query: Punishment for dowry death... ‚Üí Score: 0.943\n",
      "Query: What is culpable homicide?... ‚Üí Score: 0.956\n",
      "\n",
      "üìä Mean Faithfulness: 0.940\n",
      "   ‚úÖ Answers well-grounded in context\n"
     ]
    }
   ],
   "source": [
    "# EVALUATION 5: FAITHFULNESS\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FAITHFULNESS (Answer grounded in Context)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "faithfulness_scores = []\n",
    "\n",
    "for query in test_queries[:5]:\n",
    "    ipc_results = search_faiss(query, ipc_index, ipc_meta, top_k=2)\n",
    "    judg_results = search_faiss(query, judg_index, judg_meta, top_k=2)\n",
    "    \n",
    "    context = \"Relevant IPC Sections:\\n\"\n",
    "    for sec in ipc_results:\n",
    "        context += f\"- Section {sec['section_no']}: {sec['Description']} (Punishment: {sec['punishment_raw']})\\n\"\n",
    "    \n",
    "    context += \"\\nRelevant Judgements:\\n\"\n",
    "    for case in judg_results:\n",
    "        context += f\"- Case facts: {case['case_facts']}\\n\"\n",
    "    \n",
    "    prompt = f\"User Query: {query}\\n\\n{context}\\n\\nBased on the above IPC sections, Name the sections relating to the user query and also list possible punishments.\"\n",
    "    answer = llm._call(prompt)\n",
    "    \n",
    "    answer_emb = eval_model.encode(answer, convert_to_tensor=True)\n",
    "    context_emb = eval_model.encode(context, convert_to_tensor=True)\n",
    "    similarity = util.cos_sim(answer_emb, context_emb).item()\n",
    "    \n",
    "    faithfulness_scores.append(similarity)\n",
    "    print(f\"Query: {query[:50]}... ‚Üí Score: {similarity:.3f}\")\n",
    "\n",
    "print(f\"\\nüìä Mean Faithfulness: {np.mean(faithfulness_scores):.3f}\")\n",
    "\n",
    "if np.mean(faithfulness_scores) > 0.7:\n",
    "    print(f\"   ‚úÖ Answers well-grounded in context\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è Possible hallucination detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e02985f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "BERTSCORE\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "226eed2c0ebf4bb9bd34682c17798d60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\ml\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\nihca\\.cache\\huggingface\\hub\\models--roberta-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bd48f312eab41a687d83e96f966e060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08aa08caa7714c6290aee6dfe1a40377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f1a18e280364a38ae03a49987980bc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8399b0db7ef40b393ac9e7671d9ab5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd4f2ba0020b499d991655c087d59869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä BERTScore (Answer vs Context):\n",
      "   Precision: 0.834\n",
      "   Recall: 0.852\n",
      "   F1: 0.843\n",
      "   ‚úÖ GOOD semantic similarity\n"
     ]
    }
   ],
   "source": [
    "# EVALUATION 6: BERTSCORE\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BERTSCORE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    from bert_score import score as bert_score\n",
    "    \n",
    "    generated_answers = []\n",
    "    contexts = []\n",
    "    \n",
    "    for query in test_queries[:5]:\n",
    "        ipc_results = search_faiss(query, ipc_index, ipc_meta, top_k=2)\n",
    "        \n",
    "        context = \"Relevant IPC Sections:\\n\"\n",
    "        for sec in ipc_results:\n",
    "            context += f\"- Section {sec['section_no']}: {sec['Description']} (Punishment: {sec['punishment_raw']})\\n\"\n",
    "        \n",
    "        prompt = f\"User Query: {query}\\n\\n{context}\\n\\nBased on the above IPC sections, Name the sections relating to the user query and also list possible punishments.\"\n",
    "        answer = llm._call(prompt)\n",
    "        \n",
    "        generated_answers.append(answer)\n",
    "        contexts.append(context)\n",
    "    \n",
    "    P, R, F1 = bert_score(generated_answers, contexts, lang='en', verbose=False)\n",
    "    \n",
    "    print(f\"\\nüìä BERTScore (Answer vs Context):\")\n",
    "    print(f\"   Precision: {P.mean().item():.3f}\")\n",
    "    print(f\"   Recall: {R.mean().item():.3f}\")\n",
    "    print(f\"   F1: {F1.mean().item():.3f}\")\n",
    "    \n",
    "    if F1.mean().item() > 0.7:\n",
    "        print(f\"   ‚úÖ GOOD semantic similarity\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è MODERATE semantic similarity\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"‚ùå Install: pip install bert-score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6b5f6dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LLM-AS-JUDGE (Google Gemini)\n",
      "============================================================\n",
      "\n",
      "üí¨ Answer:  Based on your user query, the relevant Indian Penal Code (IPC) section for murder is Section 302. The punishment for this crime includes:\n",
      "\n",
      "1. Death p...\n",
      "\n",
      "‚öñÔ∏è Judge Response: ```json\n",
      "{\"relevance\": 4, \"faithfulness\": 5, \"legal_accuracy\": 4, \"completeness\": 3, \"clarity\": 5}\n",
      "```\n",
      "\n",
      "üìä Scores:\n",
      "   relevance: 4/5\n",
      "   faithfulness: 5/5\n",
      "   legal_accuracy: 4/5\n",
      "   completeness: 3/5\n",
      "   clarity: 5/5\n",
      "\n",
      "   Average: 4.20/5 ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "# EVALUATION 7: LLM-AS-JUDGE (Using Google Gemini)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LLM-AS-JUDGE (Google Gemini)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Configure Gemini API (replace with your API key)\n",
    "GEMINI_API_KEY = \"AIzaSyDbxRX-ytJxp6yDmw2rQr8IHUNWnO4nB7w\"  # Get from https://makersuite.google.com/app/apikey\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "test_query = \"What is the punishment for murder?\"\n",
    "\n",
    "ipc_results = search_faiss(test_query, ipc_index, ipc_meta, top_k=2)\n",
    "context = \"Relevant IPC Sections:\\n\"\n",
    "for sec in ipc_results:\n",
    "    context += f\"- Section {sec['section_no']}: {sec['Description']} (Punishment: {sec['punishment_raw']})\\n\"\n",
    "\n",
    "prompt = f\"User Query: {test_query}\\n\\n{context}\\n\\nBased on the above IPC sections, Name the sections relating to the user query and also list possible punishments.\"\n",
    "answer = llm._call(prompt)\n",
    "\n",
    "judge_prompt = f\"\"\"You are an expert legal evaluator. Rate this Indian legal chatbot response.\n",
    "\n",
    "Query: {test_query}\n",
    "Context: {context}\n",
    "Answer: {answer}\n",
    "\n",
    "Rate 1-5 for: relevance, faithfulness, legal_accuracy, completeness, clarity\n",
    "\n",
    "Respond ONLY in JSON format (no markdown, no other text):\n",
    "{{\"relevance\": 4, \"faithfulness\": 5, \"legal_accuracy\": 4, \"completeness\": 3, \"clarity\": 5}}\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    # Call Gemini\n",
    "    model = genai.GenerativeModel('gemini-2.0-flash-lite')\n",
    "    judge_response = model.generate_content(judge_prompt)\n",
    "    judge_text = judge_response.text\n",
    "    \n",
    "    print(f\"\\nüí¨ Answer: {answer[:150]}...\")\n",
    "    print(f\"\\n‚öñÔ∏è Judge Response: {judge_text}\")\n",
    "    \n",
    "    # Parse JSON\n",
    "    json_match = re.search(r'\\{.*\\}', judge_text, re.DOTALL)\n",
    "    if json_match:\n",
    "        scores = json.loads(json_match.group())\n",
    "        print(f\"\\nüìä Scores:\")\n",
    "        for key, value in scores.items():\n",
    "            print(f\"   {key}: {value}/5\")\n",
    "        avg_score = np.mean(list(scores.values()))\n",
    "        print(f\"\\n   Average: {avg_score:.2f}/5 {'‚úÖ' if avg_score >= 3.5 else '‚ö†Ô∏è'}\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è Could not parse JSON from response\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Gemini API Error: {e}\")\n",
    "    print(\"   Make sure you've set GEMINI_API_KEY and installed: pip install google-generativeai\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
