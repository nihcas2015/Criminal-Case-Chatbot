{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "349d4443",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.chains import ConversationChain\n",
    "from typing import Optional, List, Any\n",
    "import faiss\n",
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30d4ba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer(\"bhavyagiri/InLegal-Sbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db008976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_index_and_meta(index_path, meta_path):\n",
    "    index = faiss.read_index(index_path)\n",
    "    with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        meta = json.load(f)\n",
    "    return index, meta\n",
    "\n",
    "def search_faiss(query, index, meta, top_k=2):\n",
    "    query_vec = embedder.encode([query])\n",
    "    query_vec = np.array(query_vec).astype(\"float32\")\n",
    "    D, I = index.search(query_vec, top_k)\n",
    "    results = []\n",
    "    for idx in I[0]:\n",
    "        if idx < len(meta):\n",
    "            results.append(meta[idx])\n",
    "    return results\n",
    "class ollama(LLM):\n",
    "    model_name: str = \"mistral\"\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"ollama\"\n",
    "        \n",
    "    def _call(self,prompt: str, stop: Optional[List[str]] = None, **kwargs: Any):\n",
    "        response = requests.post(\n",
    "            \"http://127.0.0.1:11434/api/generate\",\n",
    "            json={\"model\": self.model_name, \"prompt\": prompt, \"stream\": False}\n",
    "        )\n",
    "        return response.json()[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d924f68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipc_index, ipc_meta = load_index_and_meta(\"ipc.index\", \"ipc.json\")\n",
    "judg_index, judg_meta = load_index_and_meta(\"judgments.index\", \"judgments.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d2f7be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom=PromptTemplate(input_variables=[\"history\",\"input\"],template=\"\"\" You are an legal Chatbot assistant specializing in India Penal Code.\n",
    "                      \n",
    "                      Previous conversations:{history}\n",
    "                      \n",
    "                      Current question and also judgements :{input}\n",
    "                      \n",
    "                    provide an suitable reply for the current question by utilizing previous conversations and keep it around 20 to 30 words .\n",
    "                      Utilize similar case judgement and compare it with the user case summary and then reply but do not repeat about judgements in every replies.\n",
    "                      Only use the given text as knowledge, Do Not Retrieve your own knowledge\n",
    "                      Only Reply using the given IPC sections, STICK TO ONLY THIS TEXT AND PREVIOUS CONVERSATION FOR KNOWLEDGE.\n",
    "                      Consider youself as an legal advisor and you are the professional.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bbc7d832",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ollama()\n",
    "memory=ConversationBufferMemory()\n",
    "conversion = ConversationChain(llm=llm,memory=memory,prompt=custom,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "875ecf6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öñÔ∏è IPC + Judgement Chatbot (type 'exit' to quit)\n",
      "\n",
      "\n",
      "Bot:  Based on the user's query, it seems like there might be a combination of offenses that could apply in this hypothetical situation. Here are the relevant IPC sections and their potential punishments:\n",
      "\n",
      "1. Section 300 (Murder): This section applies when a person causes someone's death with the intention to cause death or to cause such bodily injury as is likely to cause death, or with the knowledge that he is likely by such act to cause death. The punishment for murder is imprisonment for life or capital punishment.\n",
      "\n",
      "   However, the act of killing might not be intentional but a probable consequence of the abetment, as mentioned in Section 111. In such a case, the abettors could potentially be liable under this section.\n",
      "\n",
      "2. Section 304 Part I (Punishment for culpable homicide not amounting to murder): This section applies when a person causes someone's death without any intention to cause death or to cause such bodily injury as is likely to cause death, but with the knowledge that he is likely by his conduct to cause death. The punishment for this offense is imprisonment up to 10 years and a fine.\n",
      "\n",
      "3. Section 80 (Accident in doing a lawful act): If it can be proven that the act leading to the death was done by accident or misfortune, without any criminal intention or knowledge, then it might not be considered an offense. However, given the circumstances described in the user's query, it seems unlikely that this section would apply.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"‚öñÔ∏è IPC + Judgement Chatbot (type 'exit' to quit)\\n\")\n",
    "\n",
    "user_input = input(\"You: \").strip()\n",
    "\n",
    "\n",
    "ipc_results = search_faiss(user_input, ipc_index, ipc_meta)\n",
    "judg_results = search_faiss(user_input, judg_index, judg_meta)\n",
    "\n",
    "        # Step 2: Build context\n",
    "context = \"Relevant IPC Sections:\\n\"\n",
    "for sec in ipc_results:\n",
    "    context += f\"- Section {sec['section_no']}: {sec['Description']} (Punishment: {sec['punishment_raw']})\\n\"\n",
    "\n",
    "prompt = f\"User Query: {user_input}\\n\\n{context}\\n\\nBased on the above IPC sections, Name the sections relating to the user query and also list possible punishments.\"\n",
    "Reply_1=llm._call(prompt)\n",
    "\n",
    "context += \"\\nRelevant Judgements:\\n\"\n",
    "for case in judg_results:\n",
    "    context += f\"\"\"- Case facts: {case['case_facts']}\n",
    "Issues: {case['issues']}\n",
    "Arguments: {case['arguments']}\n",
    "Decision: {case['decision']}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nBot: {Reply_1}\\n\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca8260dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input(\"You: \").strip()\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Bot: Goodbye!\")\n",
    "        break\n",
    "\n",
    "    user_input+=f\"\\n\\n similar case judgements : {context}\"\n",
    "\n",
    "    answer = conversion.predict(input=user_input)\n",
    "\n",
    "    print(f\"\\nBot: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dde3bbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ready with 10 test queries\n",
      "‚úÖ Using evaluation model: bhavyagiri/InLegal-Sbert (your existing embedder)\n"
     ]
    }
   ],
   "source": [
    "# Test queries for evaluation\n",
    "test_queries = [\n",
    "    \"What is the punishment for murder?\",\n",
    "    \"Explain Section 302 IPC\",\n",
    "    \"What are the differences between theft and robbery?\",\n",
    "    \"Punishment for dowry death\",\n",
    "    \"What is culpable homicide?\",\n",
    "    \"Explain Section 420 IPC about cheating\",\n",
    "    \"What is the punishment for kidnapping?\",\n",
    "    \"Define Section 376 IPC\",\n",
    "    \"What are the provisions for defamation?\",\n",
    "    \"Explain attempt to murder under IPC\"\n",
    "]\n",
    "\n",
    "# Use the SAME model you're already using (no need to load a new one)\n",
    "eval_model = embedder  # Reuse your existing InLegal-Sbert model\n",
    "\n",
    "print(f\"‚úÖ Ready with {len(test_queries)} test queries\")\n",
    "print(f\"‚úÖ Using evaluation model: bhavyagiri/InLegal-Sbert (your existing embedder)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19db7f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RETRIEVAL CONSISTENCY\n",
      "============================================================\n",
      "\n",
      "Query: 'What is the punishment for murder?'\n",
      "üìä IPC Retrieval Consistent: ‚úÖ YES\n",
      "üìä Judgement Retrieval Consistent: ‚úÖ YES\n",
      "   Retrieved: ('Section302:- Punishment for murder', 'Section507:- Criminal intimidation by an anonymous communication')\n"
     ]
    }
   ],
   "source": [
    "# EVALUATION 1: RETRIEVAL CONSISTENCY\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RETRIEVAL CONSISTENCY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_query = \"What is the punishment for murder?\"\n",
    "num_runs = 5\n",
    "\n",
    "ipc_retrieved = []\n",
    "judg_retrieved = []\n",
    "\n",
    "for run in range(num_runs):\n",
    "    ipc_results = search_faiss(test_query, ipc_index, ipc_meta, top_k=2)\n",
    "    judg_results = search_faiss(test_query, judg_index, judg_meta, top_k=2)\n",
    "    \n",
    "    ipc_ids = tuple([r['section_no'] for r in ipc_results])\n",
    "    judg_ids = tuple([r['id'] for r in judg_results])\n",
    "    \n",
    "    ipc_retrieved.append(ipc_ids)\n",
    "    judg_retrieved.append(judg_ids)\n",
    "\n",
    "ipc_consistent = len(set(ipc_retrieved)) == 1\n",
    "judg_consistent = len(set(judg_retrieved)) == 1\n",
    "\n",
    "print(f\"\\nQuery: '{test_query}'\")\n",
    "print(f\"üìä IPC Retrieval Consistent: {'‚úÖ YES' if ipc_consistent else '‚ùå NO'}\")\n",
    "print(f\"üìä Judgement Retrieval Consistent: {'‚úÖ YES' if judg_consistent else '‚ùå NO'}\")\n",
    "print(f\"   Retrieved: {ipc_retrieved[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54b04f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ANSWER CONSISTENCY\n",
      "============================================================\n",
      "\n",
      "üìä Consistency Score: 0.970\n",
      "   Min Similarity: 0.964\n",
      "   Max Similarity: 0.976\n",
      "   ‚úÖ HIGH consistency\n"
     ]
    }
   ],
   "source": [
    "# EVALUATION 2: ANSWER CONSISTENCY\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANSWER CONSISTENCY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_query = \"What is the punishment for murder?\"\n",
    "num_samples = 3\n",
    "\n",
    "answers = []\n",
    "for i in range(num_samples):\n",
    "    ipc_results = search_faiss(test_query, ipc_index, ipc_meta, top_k=2)\n",
    "    \n",
    "    context = \"Relevant IPC Sections:\\n\"\n",
    "    for sec in ipc_results:\n",
    "        context += f\"- Section {sec['section_no']}: {sec['Description']} (Punishment: {sec['punishment_raw']})\\n\"\n",
    "    \n",
    "    prompt = f\"User Query: {test_query}\\n\\n{context}\\n\\nBased on the above IPC sections, Name the sections relating to the user query and also list possible punishments.\"\n",
    "    answer = llm._call(prompt)\n",
    "    answers.append(answer)\n",
    "\n",
    "embeddings = eval_model.encode(answers, convert_to_tensor=True)\n",
    "similarities = []\n",
    "\n",
    "for i in range(len(answers)):\n",
    "    for j in range(i+1, len(answers)):\n",
    "        sim = util.cos_sim(embeddings[i], embeddings[j]).item()\n",
    "        similarities.append(sim)\n",
    "\n",
    "consistency_score = np.mean(similarities)\n",
    "\n",
    "print(f\"\\nüìä Consistency Score: {consistency_score:.3f}\")\n",
    "print(f\"   Min Similarity: {np.min(similarities):.3f}\")\n",
    "print(f\"   Max Similarity: {np.max(similarities):.3f}\")\n",
    "\n",
    "if consistency_score > 0.8:\n",
    "    print(f\"   ‚úÖ HIGH consistency\")\n",
    "elif consistency_score > 0.6:\n",
    "    print(f\"   ‚ö†Ô∏è MODERATE consistency\")\n",
    "else:\n",
    "    print(f\"   ‚ùå LOW consistency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2eeb75c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CONTEXT RELEVANCE\n",
      "============================================================\n",
      "Query: What is the punishment for murder?... ‚Üí Score: 0.659\n",
      "Query: Explain Section 302 IPC... ‚Üí Score: 0.673\n",
      "Query: What are the differences between theft and robbery... ‚Üí Score: 0.743\n",
      "Query: Punishment for dowry death... ‚Üí Score: 0.698\n",
      "Query: What is culpable homicide?... ‚Üí Score: 0.680\n",
      "\n",
      "üìä Mean Context Relevance: 0.691\n",
      "   Range: [0.659, 0.743]\n"
     ]
    }
   ],
   "source": [
    "# EVALUATION 3: CONTEXT RELEVANCE\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONTEXT RELEVANCE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "relevance_scores = []\n",
    "\n",
    "for query in test_queries[:5]:\n",
    "    ipc_results = search_faiss(query, ipc_index, ipc_meta, top_k=2)\n",
    "    \n",
    "    context = \"Relevant IPC Sections:\\n\"\n",
    "    for sec in ipc_results:\n",
    "        context += f\"- Section {sec['section_no']}: {sec['Description']} (Punishment: {sec['punishment_raw']})\\n\"\n",
    "    \n",
    "    query_emb = eval_model.encode(query, convert_to_tensor=True)\n",
    "    context_emb = eval_model.encode(context, convert_to_tensor=True)\n",
    "    similarity = util.cos_sim(query_emb, context_emb).item()\n",
    "    \n",
    "    relevance_scores.append(similarity)\n",
    "    print(f\"Query: {query[:50]}... ‚Üí Score: {similarity:.3f}\")\n",
    "\n",
    "print(f\"\\nüìä Mean Context Relevance: {np.mean(relevance_scores):.3f}\")\n",
    "print(f\"   Range: [{np.min(relevance_scores):.3f}, {np.max(relevance_scores):.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7019eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ANSWER RELEVANCE\n",
      "============================================================\n",
      "Query: What is the punishment for murder?... ‚Üí Score: 0.671\n",
      "Query: Explain Section 302 IPC... ‚Üí Score: 0.619\n",
      "Query: What are the differences between theft and robbery... ‚Üí Score: 0.753\n",
      "Query: Punishment for dowry death... ‚Üí Score: 0.703\n",
      "Query: What is culpable homicide?... ‚Üí Score: 0.693\n",
      "\n",
      "üìä Mean Answer Relevance: 0.688\n",
      "   Range: [0.619, 0.753]\n"
     ]
    }
   ],
   "source": [
    "# EVALUATION 4: ANSWER RELEVANCE\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANSWER RELEVANCE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "relevance_scores = []\n",
    "\n",
    "for query in test_queries[:5]:\n",
    "    ipc_results = search_faiss(query, ipc_index, ipc_meta, top_k=2)\n",
    "    \n",
    "    context = \"Relevant IPC Sections:\\n\"\n",
    "    for sec in ipc_results:\n",
    "        context += f\"- Section {sec['section_no']}: {sec['Description']} (Punishment: {sec['punishment_raw']})\\n\"\n",
    "    \n",
    "    prompt = f\"User Query: {query}\\n\\n{context}\\n\\nBased on the above IPC sections, Name the sections relating to the user query and also list possible punishments.\"\n",
    "    answer = llm._call(prompt)\n",
    "    \n",
    "    query_emb = eval_model.encode(query, convert_to_tensor=True)\n",
    "    answer_emb = eval_model.encode(answer, convert_to_tensor=True)\n",
    "    similarity = util.cos_sim(query_emb, answer_emb).item()\n",
    "    \n",
    "    relevance_scores.append(similarity)\n",
    "    print(f\"Query: {query[:50]}... ‚Üí Score: {similarity:.3f}\")\n",
    "\n",
    "print(f\"\\nüìä Mean Answer Relevance: {np.mean(relevance_scores):.3f}\")\n",
    "print(f\"   Range: [{np.min(relevance_scores):.3f}, {np.max(relevance_scores):.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "04d3b6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FAITHFULNESS (Answer grounded in Context)\n",
      "============================================================\n",
      "Query: What is the punishment for murder?... ‚Üí Score: 0.932\n",
      "Query: Explain Section 302 IPC... ‚Üí Score: 0.926\n",
      "Query: What are the differences between theft and robbery... ‚Üí Score: 0.945\n",
      "Query: Punishment for dowry death... ‚Üí Score: 0.943\n",
      "Query: What is culpable homicide?... ‚Üí Score: 0.956\n",
      "\n",
      "üìä Mean Faithfulness: 0.940\n",
      "   ‚úÖ Answers well-grounded in context\n"
     ]
    }
   ],
   "source": [
    "# EVALUATION 5: FAITHFULNESS\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FAITHFULNESS (Answer grounded in Context)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "faithfulness_scores = []\n",
    "\n",
    "for query in test_queries[:5]:\n",
    "    ipc_results = search_faiss(query, ipc_index, ipc_meta, top_k=2)\n",
    "    judg_results = search_faiss(query, judg_index, judg_meta, top_k=2)\n",
    "    \n",
    "    context = \"Relevant IPC Sections:\\n\"\n",
    "    for sec in ipc_results:\n",
    "        context += f\"- Section {sec['section_no']}: {sec['Description']} (Punishment: {sec['punishment_raw']})\\n\"\n",
    "    \n",
    "    context += \"\\nRelevant Judgements:\\n\"\n",
    "    for case in judg_results:\n",
    "        context += f\"- Case facts: {case['case_facts']}\\n\"\n",
    "    \n",
    "    prompt = f\"User Query: {query}\\n\\n{context}\\n\\nBased on the above IPC sections, Name the sections relating to the user query and also list possible punishments.\"\n",
    "    answer = llm._call(prompt)\n",
    "    \n",
    "    answer_emb = eval_model.encode(answer, convert_to_tensor=True)\n",
    "    context_emb = eval_model.encode(context, convert_to_tensor=True)\n",
    "    similarity = util.cos_sim(answer_emb, context_emb).item()\n",
    "    \n",
    "    faithfulness_scores.append(similarity)\n",
    "    print(f\"Query: {query[:50]}... ‚Üí Score: {similarity:.3f}\")\n",
    "\n",
    "print(f\"\\nüìä Mean Faithfulness: {np.mean(faithfulness_scores):.3f}\")\n",
    "\n",
    "if np.mean(faithfulness_scores) > 0.7:\n",
    "    print(f\"   ‚úÖ Answers well-grounded in context\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è Possible hallucination detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e02985f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "BERTSCORE\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "226eed2c0ebf4bb9bd34682c17798d60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\ml\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\nihca\\.cache\\huggingface\\hub\\models--roberta-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bd48f312eab41a687d83e96f966e060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08aa08caa7714c6290aee6dfe1a40377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f1a18e280364a38ae03a49987980bc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8399b0db7ef40b393ac9e7671d9ab5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd4f2ba0020b499d991655c087d59869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä BERTScore (Answer vs Context):\n",
      "   Precision: 0.834\n",
      "   Recall: 0.852\n",
      "   F1: 0.843\n",
      "   ‚úÖ GOOD semantic similarity\n"
     ]
    }
   ],
   "source": [
    "# EVALUATION 6: BERTSCORE\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BERTSCORE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    from bert_score import score as bert_score\n",
    "    \n",
    "    generated_answers = []\n",
    "    contexts = []\n",
    "    \n",
    "    for query in test_queries[:5]:\n",
    "        ipc_results = search_faiss(query, ipc_index, ipc_meta, top_k=2)\n",
    "        \n",
    "        context = \"Relevant IPC Sections:\\n\"\n",
    "        for sec in ipc_results:\n",
    "            context += f\"- Section {sec['section_no']}: {sec['Description']} (Punishment: {sec['punishment_raw']})\\n\"\n",
    "        \n",
    "        prompt = f\"User Query: {query}\\n\\n{context}\\n\\nBased on the above IPC sections, Name the sections relating to the user query and also list possible punishments.\"\n",
    "        answer = llm._call(prompt)\n",
    "        \n",
    "        generated_answers.append(answer)\n",
    "        contexts.append(context)\n",
    "    \n",
    "    P, R, F1 = bert_score(generated_answers, contexts, lang='en', verbose=False)\n",
    "    \n",
    "    print(f\"\\nüìä BERTScore (Answer vs Context):\")\n",
    "    print(f\"   Precision: {P.mean().item():.3f}\")\n",
    "    print(f\"   Recall: {R.mean().item():.3f}\")\n",
    "    print(f\"   F1: {F1.mean().item():.3f}\")\n",
    "    \n",
    "    if F1.mean().item() > 0.7:\n",
    "        print(f\"   ‚úÖ GOOD semantic similarity\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è MODERATE semantic similarity\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"‚ùå Install: pip install bert-score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6b5f6dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LLM-AS-JUDGE (Google Gemini)\n",
      "============================================================\n",
      "\n",
      "üí¨ Answer:  Based on your user query, the relevant Indian Penal Code (IPC) section for murder is Section 302. The punishment for this crime includes:\n",
      "\n",
      "1. Death p...\n",
      "\n",
      "‚öñÔ∏è Judge Response: ```json\n",
      "{\"relevance\": 4, \"faithfulness\": 5, \"legal_accuracy\": 4, \"completeness\": 3, \"clarity\": 5}\n",
      "```\n",
      "\n",
      "üìä Scores:\n",
      "   relevance: 4/5\n",
      "   faithfulness: 5/5\n",
      "   legal_accuracy: 4/5\n",
      "   completeness: 3/5\n",
      "   clarity: 5/5\n",
      "\n",
      "   Average: 4.20/5 ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "# EVALUATION 7: LLM-AS-JUDGE (Using Google Gemini)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LLM-AS-JUDGE (Google Gemini)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Configure Gemini API (replace with your API key)\n",
    "GEMINI_API_KEY = \"AIzaSyDbxRX-ytJxp6yDmw2rQr8IHUNWnO4nB7w\"  # Get from https://makersuite.google.com/app/apikey\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "test_query = \"What is the punishment for murder?\"\n",
    "\n",
    "ipc_results = search_faiss(test_query, ipc_index, ipc_meta, top_k=2)\n",
    "context = \"Relevant IPC Sections:\\n\"\n",
    "for sec in ipc_results:\n",
    "    context += f\"- Section {sec['section_no']}: {sec['Description']} (Punishment: {sec['punishment_raw']})\\n\"\n",
    "\n",
    "prompt = f\"User Query: {test_query}\\n\\n{context}\\n\\nBased on the above IPC sections, Name the sections relating to the user query and also list possible punishments.\"\n",
    "answer = llm._call(prompt)\n",
    "\n",
    "judge_prompt = f\"\"\"You are an expert legal evaluator. Rate this Indian legal chatbot response.\n",
    "\n",
    "Query: {test_query}\n",
    "Context: {context}\n",
    "Answer: {answer}\n",
    "\n",
    "Rate 1-5 for: relevance, faithfulness, legal_accuracy, completeness, clarity\n",
    "\n",
    "Respond ONLY in JSON format (no markdown, no other text):\n",
    "{{\"relevance\": 4, \"faithfulness\": 5, \"legal_accuracy\": 4, \"completeness\": 3, \"clarity\": 5}}\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    # Call Gemini\n",
    "    model = genai.GenerativeModel('gemini-2.0-flash-lite')\n",
    "    judge_response = model.generate_content(judge_prompt)\n",
    "    judge_text = judge_response.text\n",
    "    \n",
    "    print(f\"\\nüí¨ Answer: {answer[:150]}...\")\n",
    "    print(f\"\\n‚öñÔ∏è Judge Response: {judge_text}\")\n",
    "    \n",
    "    # Parse JSON\n",
    "    json_match = re.search(r'\\{.*\\}', judge_text, re.DOTALL)\n",
    "    if json_match:\n",
    "        scores = json.loads(json_match.group())\n",
    "        print(f\"\\nüìä Scores:\")\n",
    "        for key, value in scores.items():\n",
    "            print(f\"   {key}: {value}/5\")\n",
    "        avg_score = np.mean(list(scores.values()))\n",
    "        print(f\"\\n   Average: {avg_score:.2f}/5 {'‚úÖ' if avg_score >= 3.5 else '‚ö†Ô∏è'}\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è Could not parse JSON from response\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Gemini API Error: {e}\")\n",
    "    print(\"   Make sure you've set GEMINI_API_KEY and installed: pip install google-generativeai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "88f781f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-2.5-pro-preview-03-25\n",
      "models/gemini-2.5-flash-preview-05-20\n",
      "models/gemini-2.5-flash\n",
      "models/gemini-2.5-flash-lite-preview-06-17\n",
      "models/gemini-2.5-pro-preview-05-06\n",
      "models/gemini-2.5-pro-preview-06-05\n",
      "models/gemini-2.5-pro\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-preview-image-generation\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/gemini-2.5-flash-preview-tts\n",
      "models/gemini-2.5-pro-preview-tts\n",
      "models/learnlm-2.0-flash-experimental\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n",
      "models/gemma-3n-e4b-it\n",
      "models/gemma-3n-e2b-it\n",
      "models/gemini-flash-latest\n",
      "models/gemini-flash-lite-latest\n",
      "models/gemini-pro-latest\n",
      "models/gemini-2.5-flash-lite\n",
      "models/gemini-2.5-flash-image-preview\n",
      "models/gemini-2.5-flash-image\n",
      "models/gemini-2.5-flash-preview-09-2025\n",
      "models/gemini-2.5-flash-lite-preview-09-2025\n",
      "models/gemini-robotics-er-1.5-preview\n",
      "models/gemini-2.5-computer-use-preview-10-2025\n"
     ]
    }
   ],
   "source": [
    "for model in genai.list_models():\n",
    "    if 'generateContent' in model.supported_generation_methods:\n",
    "        print(model.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8efb35f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ollama(LLM):\n",
    "    model_name: str = \"mistral\"\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"ollama\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None, *args, **kwargs: Any):\n",
    "        import requests\n",
    "        response = requests.post(\n",
    "            \"http://127.0.0.1:11434/api/generate\",\n",
    "            json={\"model\": self.model_name, \"prompt\": prompt, \"stream\": False},\n",
    "            timeout=4000\n",
    "\n",
    "        )\n",
    "        return response.json()[\"response\"]\n",
    "llm=ollama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "016918e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RAGAS FRAMEWORK\n",
      "============================================================\n",
      "‚è≥ Evaluating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2c4758e1b84413db7a9bc90299cf11d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[0]: TimeoutError()\n",
      "Exception raised in Job[2]: TimeoutError()\n",
      "Exception raised in Job[6]: TimeoutError()\n",
      "Exception raised in Job[9]: TimeoutError()\n",
      "Exception raised in Job[8]: TimeoutError()\n",
      "Exception raised in Job[5]: TimeoutError()\n",
      "Exception raised in Job[7]: TimeoutError()\n",
      "Exception raised in Job[4]: TimeoutError()\n",
      "Exception raised in Job[1]: TimeoutError()\n",
      "Exception raised in Job[3]: TimeoutError()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä RAGAS Results:\n",
      "   Faithfulness:      nan\n",
      "   Answer Relevancy:  nan\n",
      "\n",
      "   Average: nan ‚ö†Ô∏è\n"
     ]
    }
   ],
   "source": [
    "# EVALUATION 8: RAGAS\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RAGAS FRAMEWORK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    from ragas import evaluate\n",
    "    from ragas.metrics import faithfulness, answer_relevancy\n",
    "    from datasets import Dataset\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "    # ‚úÖ Use your custom sentence embedder\n",
    "    embedder = SentenceTransformer(\"bhavyagiri/InLegal-Sbert\")\n",
    "\n",
    "    # --- Optional: wrap it for RAGAS compatibility (if it expects LangChain embedding interface)\n",
    "    class CustomEmbedder:\n",
    "        def embed_documents(self, texts):\n",
    "            return embedder.encode(texts, convert_to_numpy=True).tolist()\n",
    "        \n",
    "        def embed_query(self, text):\n",
    "            return embedder.encode([text], convert_to_numpy=True).tolist()[0]\n",
    "\n",
    "    embedding_model = CustomEmbedder()\n",
    "\n",
    "    # ‚úÖ Create your data\n",
    "    data = {'question': [], 'contexts': [], 'answer': []}\n",
    "\n",
    "    for query in test_queries[:5]:\n",
    "        ipc_results = search_faiss(query, ipc_index, ipc_meta, top_k=2)\n",
    "\n",
    "        context = \"Relevant IPC Sections:\\n\"\n",
    "        for sec in ipc_results:\n",
    "            context += f\"- Section {sec['section_no']}: {sec['Description']} (Punishment: {sec['punishment_raw']})\\n\"\n",
    "\n",
    "        prompt = (\n",
    "            f\"User Query: {query}\\n\\n{context}\\n\\n\"\n",
    "            \"Based on the above IPC sections, name the relevant sections and list possible punishments.\"\n",
    "        )\n",
    "        answer = llm._call(prompt)\n",
    "\n",
    "        data['question'].append(query)\n",
    "        data['contexts'].append([context])\n",
    "        data['answer'].append(answer)\n",
    "\n",
    "    dataset = Dataset.from_dict(data)\n",
    "\n",
    "    print(\"‚è≥ Evaluating...\")\n",
    "\n",
    "    # ‚úÖ Pass custom embedding model explicitly\n",
    "    result = evaluate(\n",
    "        dataset,\n",
    "        metrics=[faithfulness, answer_relevancy],\n",
    "        llm=llm,\n",
    "        embeddings=embedding_model\n",
    "    )\n",
    "\n",
    "    faith = result[\"faithfulness\"]\n",
    "    relev = result[\"answer_relevancy\"]\n",
    "\n",
    "# If they're lists, take the mean\n",
    "    if isinstance(faith, list):\n",
    "        faith = sum(faith) / len(faith)\n",
    "    if isinstance(relev, list):\n",
    "        relev = sum(relev) / len(relev)\n",
    "\n",
    "    print(\"\\nüìä RAGAS Results:\")\n",
    "    print(f\"   Faithfulness:      {faith:.3f}\")\n",
    "    print(f\"   Answer Relevancy:  {relev:.3f}\")\n",
    "\n",
    "    avg = (faith + relev) / 2\n",
    "    print(f\"\\n   Average: {avg:.3f} {'‚úÖ' if avg > 0.7 else '‚ö†Ô∏è'}\")\n",
    "\n",
    "\n",
    "except ImportError:\n",
    "    print(\"‚ùå Install required packages: pip install ragas datasets sentence-transformers langchain\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "29e448bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "‚úÖ EVALUATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Completed Evaluations:\n",
      "‚úÖ Retrieval Consistency\n",
      "‚úÖ Answer Consistency  \n",
      "‚úÖ Context Relevance\n",
      "‚úÖ Answer Relevance\n",
      "‚úÖ Faithfulness\n",
      "‚úÖ BERTScore\n",
      "‚úÖ LLM-as-Judge\n",
      "‚úÖ RAGAS\n",
      "\n",
      "Review scores above. Scores < 0.6 need improvement.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# FINAL SUMMARY\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ EVALUATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\"\"\n",
    "Completed Evaluations:\n",
    "‚úÖ Retrieval Consistency\n",
    "‚úÖ Answer Consistency  \n",
    "‚úÖ Context Relevance\n",
    "‚úÖ Answer Relevance\n",
    "‚úÖ Faithfulness\n",
    "‚úÖ BERTScore\n",
    "‚úÖ LLM-as-Judge\n",
    "‚úÖ RAGAS\n",
    "\n",
    "Review scores above. Scores < 0.6 need improvement.\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
